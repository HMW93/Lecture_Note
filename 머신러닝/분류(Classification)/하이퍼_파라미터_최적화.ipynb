{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "하이퍼 파라미터 최적화ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLR3HxPj1fep"
      },
      "source": [
        "# iris_data 파라미터 최적화 방법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb3PfgOw7DUI"
      },
      "source": [
        "#import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "#Loading iris dataset from sklearn\n",
        "iris = load_iris()\n",
        "\n",
        "#independent feautres\n",
        "X = iris.data\n",
        "\n",
        "# target features\n",
        "y = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7SWSd_N7VJI"
      },
      "source": [
        "#import XGboost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#Defining XGB Classification model\n",
        "clf = XGBClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b4MwEbA7c6o"
      },
      "source": [
        "# 1.Grid SearchCV\n",
        "- 사용자가 하이퍼 파라미터마다 몇가지 값을 가진 리스트를 입력하면, 가능한 하이퍼 파라미터의 경우의 수마다 예측 성능을 측정하여 사용자가 일일이 하이퍼 파라미터를 설정하고, 예측 성능을 비교하여 최적의 파라미터를 찾는 수고를 줄이고 이 과정을 한꺼번에 진행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8j47oJZ7cYv"
      },
      "source": [
        "#Importing packages from sklearn\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "\n",
        "#defining a set of values as a dictionary for hyperparameters\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\":[100,200,300,400],\n",
        "    \"max_depth\":[1,3,5,7],\n",
        "    \"reg_lambda\":[.01,.1,.5]    \n",
        "}\n",
        "\n",
        "#declaring GridSearchCV model\n",
        "\n",
        "model = model_selection.GridSearchCV(\n",
        "    estimator = clf,\n",
        "    param_grid = param_grid,\n",
        "    scoring = 'accuracy',\n",
        "    verbose = 10,\n",
        "    n_jobs = 1,\n",
        "    cv = 5    \n",
        ")\n",
        "\n",
        "#fitting values to the gridsearchcv model\n",
        "\n",
        "model.fit(X,y)\n",
        "#printing the best possible values to enhance accuracy\n",
        "print(model.best_params_)\n",
        "print(model.best_estimator_)\n",
        "#printing the best score\n",
        "print(model.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utwaZq0g7o7o"
      },
      "source": [
        "# 2. RandomizedSearchCV\n",
        "\n",
        "-그리드 서치에서는 grid_param과 같이 매개변수마다 특정 값을 지정해주었습니다. 만약에 변수 범위가 너무 다양하다면 하나하나 작성해주는게 너무 힘들다.\n",
        "\n",
        "- 하이퍼 파라미터 검색 반영이 너무 클때 사용하는 방식이 Randomized Search입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y6qy6zT7nxh"
      },
      "source": [
        "#defining a set of values as a dictionary for hyperparameters\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\":[100,200,300,400],\n",
        "    \"max_depth\":[1,3,5,7],\n",
        "    \"reg_lambda\":[.01,.1,.5]    \n",
        "}\n",
        "\n",
        "#declaring RandomizedSearchCV model\n",
        "\n",
        "model = model_selection.RandomizedSearchCV(\n",
        "    estimator = clf,\n",
        "    param_distributions = param_grid,\n",
        "    scoring = 'accuracy',\n",
        "    verbose = 10,\n",
        "    n_jobs = 1,\n",
        "    cv = 5,\n",
        "    n_iter=10\n",
        ")\n",
        "\n",
        "#fitting values to the RandomizedSearchCV model\n",
        "\n",
        "model.fit(X,y)\n",
        "\n",
        "#printing the best possible values to enhance accuracy\n",
        "\n",
        "print(model.best_params_)\n",
        "print(model.best_estimator_)\n",
        "#printing the best score\n",
        "print(model.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTNdO2Kf7t9w"
      },
      "source": [
        "# 3.Bayesian Optimization\n",
        "\n",
        "- 미지의 함수(Black-box function이라고 부른다)가 반환하는 값의 최대(또는 최소)값을 짧은 반복을 통해서 찾아내는 최적화 방식이다.\n",
        "\n",
        "참고 : https://techblog-history-younghunjo1.tistory.com/141"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxjoLz247z5g"
      },
      "source": [
        "!pip install scikit-optimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H06cIZ9M7tRw"
      },
      "source": [
        "#importing packages \n",
        "\n",
        "from functools import partial\n",
        "from skopt import space\n",
        "from skopt import gp_minimize\n",
        "\n",
        "#defining a method that will perfrom a 5 split cross validation over\n",
        "#dataset and and will produce the optimum value of the accuracy\n",
        "\n",
        "def optimize(params, param_names, x,y):\n",
        "\n",
        "    params = dict(zip(param_names,params))\n",
        "\n",
        "    model = XGBClassifier(**params)\n",
        "\n",
        "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    for idx in kf.split(X=x,y=y):\n",
        "\n",
        "        train_idx,test_idx = idx[0],idx[1]\n",
        "\n",
        "        xtrain = x[train_idx]\n",
        "\n",
        "        ytrain = y[train_idx]\n",
        "\n",
        "        xtest = x[test_idx]\n",
        "\n",
        "        ytest = y[test_idx]\n",
        "\n",
        "        model.fit(xtrain,ytrain)\n",
        "\n",
        "        preds =  model.predict(xtest)\n",
        "\n",
        "        fold_acc = metrics.accuracy_score(ytest,preds)\n",
        "\n",
        "        accuracies.append(fold_acc)\n",
        "\n",
        "    return -1.0 * np.mean(accuracies)\n",
        "\n",
        "#defining a set of values as space for hyperparameters\n",
        "\n",
        "param_space = [\n",
        "    space.Integer(3,15, name = \"max_depth\"),\n",
        "    space.Integer(100,600, name = \"n_estimators\"),\n",
        "    space.Real(0.01,1,prior='uniform', name=\"reg_lambda\"),\n",
        "    space.Real(0.01,1,prior='uniform', name=\"max_features\")\n",
        "]\n",
        "\n",
        "#DEfining the parameter names\n",
        "\n",
        "param_names = [\n",
        "\n",
        "    \"max_depth\",\n",
        "\n",
        "    \"n_estimators\",\n",
        "\n",
        "    \"reg_lambda\",\n",
        "\n",
        "    \"max_features\"\n",
        "\n",
        "]\n",
        "\n",
        "#defiing optimization_fuction as partial and calling optimize within it\n",
        "\n",
        "optimization_fuction = partial(optimize, param_names = param_names,x = X, y = y) \n",
        "\n",
        "#Getting the optimum values for hyperparameters\n",
        "\n",
        "result = gp_minimize(\n",
        "\n",
        "    optimization_fuction,\n",
        "\n",
        "    dimensions=param_space,\n",
        "\n",
        "    n_calls = 15,\n",
        "\n",
        "    n_random_starts= 10,\n",
        "\n",
        "    verbose= 10\n",
        "\n",
        ")\n",
        "\n",
        "#Printing the best hyperparemeter set\n",
        "\n",
        "print(dict(zip(param_names,result.x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojrxHmiQ9fbR"
      },
      "source": [
        "!pip install bayesian-optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQT-HRoW-Bhh"
      },
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import xgboost as xgb\n",
        "\n",
        "# MAPE Metric\n",
        "def mean_absolute_percentage_error(y_test, y_pred):\n",
        "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "# 탐색 대상 함수 (XGBclassifier)\n",
        "def XGB_cv(max_depth,learning_rate, n_estimators, gamma\n",
        "            ,min_child_weight, subsample\n",
        "            ,colsample_bytree, silent=True, nthread=-1):\n",
        "\n",
        "    # 모델 정의\n",
        "    model = xgb.XGBClassifier(max_depth=int(max_depth),\n",
        "                              learning_rate=learning_rate,\n",
        "                              n_estimators=int(n_estimators),\n",
        "                              gamma=gamma,\n",
        "                              min_child_weight=min_child_weight,\n",
        "                              subsample=subsample,\n",
        "                              colsample_bytree=colsample_bytree, \n",
        "                              nthread=nthread\n",
        "                              )\n",
        "    # 모델 훈련\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 예측값 출력\n",
        "    y_pred= model.predict(X_test)\n",
        "\n",
        "    # 각종 metric 계산\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "\n",
        "    # 오차 최적화로 사용할 metric 반환\n",
        "    return r2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrM9flfx9IfS"
      },
      "source": [
        " #  bayesian-optimization 라이브러리의 BayesianOptimization 클래스 import\n",
        "from bayes_opt import BayesianOptimization\n",
        "import numpy as np\n",
        "\n",
        "# 실험해보고자하는 hyperparameter 집합\n",
        "pbounds = {'max_depth': (3, 7),\n",
        "              'learning_rate': (0.01, 0.2),\n",
        "              'n_estimators': (5000, 10000),\n",
        "              'gamma': (0, 100),\n",
        "              'min_child_weight': (0, 3),\n",
        "              'subsample': (0.5, 1),\n",
        "              'colsample_bytree' :(0.2, 1)\n",
        "              }\n",
        "\n",
        "# Bayesian optimization 객체 생성\n",
        "# f : 탐색 대상 함수, pbounds : hyperparameter 집합\n",
        "# verbose = 2 항상 출력, verbose = 1 최댓값일 때 출력, verbose = 0 출력 안함\n",
        "# random_state : Bayesian Optimization 상의 랜덤성이 존재하는 부분을 통제 \n",
        "bo=BayesianOptimization(f=XGB_cv, pbounds=pbounds, verbose=2, random_state=1 )    \n",
        "\n",
        "# 메소드를 이용해 최대화 과정 수행\n",
        "# init_points :  초기 Random Search 갯수\n",
        "# n_iter : 반복 횟수 (몇개의 입력값-함숫값 점들을 확인할지! 많을 수록 정확한 값을 얻을 수 있다.)\n",
        "# acq : Acquisition Function들 중 Expected Improvement(EI) 를 사용\n",
        "# xi : exploration 강도 (기본값은 0.0)\n",
        "bo.maximize(init_points=2, n_iter=10, acq='ei', xi=0.01)\n",
        "\n",
        "# ‘iter’는 반복 회차, ‘target’은 목적 함수의 값, 나머지는 입력값을 나타냅니다. \n",
        "# 현재 회차 이전까지 조사된 함숫값들과 비교하여, 현재 회차에 최댓값이 얻어진 경우, \n",
        "# bayesian-optimization 라이브러리는 이를 자동으로 다른 색 글자로 표시하는 것을 확인할 수 있습니다\n",
        "\n",
        "# 찾은 파라미터 값 확인\n",
        "print(bo.max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJICPdtp8E2Q"
      },
      "source": [
        "# 4.Hyperopt\n",
        "\n",
        "- HyperOpt 는 하이퍼 파라미터 최적화를 위해 검색 공간 , 손실 함수 , 최적화 알고리즘 및 기록 ( 점수, 구성 ) 을 저장하기 위한 데이터베이스의 4 가지 필수 구성 요소가 필요합니다 . 검색 공간은 연속하고, 볼록 함수에 의해 결정될 것이다.\n",
        "\n",
        "참고 : https://ichi.pro/ko/hyperopt-beijian-choejeoghwaleul-giban-eulo-han-haipeo-palamiteo-tyuning-140338828128041"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHCb9jCY7xZg"
      },
      "source": [
        "#importing packages \n",
        "\n",
        "from hyperopt import hp,fmin, tpe, Trials\n",
        "\n",
        "from hyperopt.pyll.base import scope\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "from skopt import space\n",
        "\n",
        "from skopt import gp_minimize\n",
        "\n",
        "#defining a method that will perfrom a 5 split cross validation over\n",
        "\n",
        "#dataset and and will produce the optimum value of the accuracy\n",
        "\n",
        "def optimize(params, x,y):\n",
        "\n",
        "    clf = XGBClassifier(**params)\n",
        "\n",
        "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    for idx in kf.split(X=x,y=y):\n",
        "\n",
        "        train_idx,test_idx = idx[0],idx[1]\n",
        "\n",
        "        xtrain = x[train_idx]\n",
        "\n",
        "        ytrain = y[train_idx]\n",
        "\n",
        "        xtest = x[test_idx]\n",
        "\n",
        "        ytest = y[test_idx]\n",
        "\n",
        "        clf.fit(xtrain,ytrain)\n",
        "\n",
        "        preds =  clf.predict(xtest)\n",
        "\n",
        "        fold_acc = metrics.accuracy_score(ytest,preds)\n",
        "\n",
        "        accuracies.append(fold_acc)\n",
        "\n",
        "    return -1.0 * np.mean(accuracies)\n",
        "\n",
        "#defining a set of values as hp for hyperparameters\n",
        "\n",
        "param_space = {\n",
        "\n",
        "    \"max_depth\" : scope.int(hp.quniform(\"max_depth\",3,20, 1)) ,\n",
        "\n",
        "    \"min_child_weight\" : scope.int(hp.quniform(\"min_child_weight\",1,8, 1)),\n",
        "\n",
        "    \"n_estimators\": scope.int(hp.quniform(\"n_estimators\",100,1500,1)),\n",
        "\n",
        "    'learning_rate': hp.uniform(\"learning_rate\",0.01,1),\n",
        "\n",
        "    'reg_lambda': hp.uniform(\"reg_lambda\",0.01,1),\n",
        "\n",
        "    'gamma': hp.uniform(\"gamma\",0.01,1),\n",
        "\n",
        "    'subsample': hp.uniform(\"subsample\",0.01,1)\n",
        "\n",
        "    }\n",
        "\n",
        "#defiing optimization_fuction as partial and calling optimize within it\n",
        "\n",
        "optimization_fuction = partial(optimize,x = X, y = y) \n",
        "\n",
        "trials = Trials()\n",
        "\n",
        "#Getting the optimum values for hyperparameters\n",
        "\n",
        "result = fmin(\n",
        "\n",
        "    fn = optimization_fuction,\n",
        "\n",
        "    space = param_space,\n",
        "\n",
        "    algo = tpe.suggest,\n",
        "\n",
        "    max_evals = 15,\n",
        "\n",
        "    trials = trials\n",
        "\n",
        ")\n",
        "\n",
        "#Printing the best hyperparemeter set\n",
        "\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQWHSPbq8KHB"
      },
      "source": [
        "# 5. Optuna\n",
        "\n",
        "참고 : https://rosypark.tistory.com/153"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0vcDnBi8JD3"
      },
      "source": [
        "#importing packages\n",
        "\n",
        "import optuna\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "#defining a method that will perfrom a 5 split cross validation over\n",
        "\n",
        "#dataset and and will produce the optimum value of the accuracy\n",
        "\n",
        "def optimize(trial, x,y):\n",
        "\n",
        "    #parameter set is declare within function\n",
        "\n",
        "    reg_lambda = trial.suggest_uniform('reg_lambda',0.01,1)\n",
        "\n",
        "    n_estimators = trial.suggest_int('n_estimators',100,1500)\n",
        "\n",
        "    max_depth = trial.suggest_int('max_depth',3,15)\n",
        "\n",
        "    max_features = trial.suggest_uniform('max_features',0.01,1)\n",
        "\n",
        "    clf = XGBClassifier(\n",
        "\n",
        "    n_estimators= n_estimators,\n",
        "\n",
        "    reg_lambda=reg_lambda,\n",
        "\n",
        "    max_depth=max_depth,\n",
        "\n",
        "    max_features= max_features)\n",
        "\n",
        "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    for idx in kf.split(X=x,y=y):\n",
        "\n",
        "        train_idx,test_idx = idx[0],idx[1]\n",
        "\n",
        "        xtrain = x[train_idx]\n",
        "\n",
        "        ytrain = y[train_idx]\n",
        "\n",
        "        xtest = x[test_idx]\n",
        "\n",
        "        ytest = y[test_idx]\n",
        "\n",
        "        clf.fit(xtrain,ytrain)\n",
        "\n",
        "        preds =  clf.predict(xtest)\n",
        "\n",
        "        fold_acc = metrics.accuracy_score(ytest,preds)\n",
        "\n",
        "        accuracies.append(fold_acc)\n",
        "\n",
        "    return -1.0 * np.mean(accuracies)\n",
        "\n",
        "#defiing optimization_fuction as partial and calling optimize within it\n",
        "\n",
        "optimization_fuction = partial(optimize,x = X, y = y) \n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "\n",
        "#Printing the best hyperparemeter set\n",
        "\n",
        "study.optimize(optimization_fuction, n_trials=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcXuNtGJ8XVh"
      },
      "source": [
        "# 6.Pycaret"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqlGmSqx8aeh"
      },
      "source": [
        "!pip install pycaret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G4-MhRH8W3R"
      },
      "source": [
        "import optuna\n",
        "import sklearn\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# 1. 최소화/최대화할 목적함수 정의\n",
        "def objective(trial):\n",
        "    iris = sklearn.datasets.load_iris()\n",
        "    x, y = iris.data, iris.target\n",
        "\n",
        "# 2. trial object로 하이퍼파라미터 값 추천\n",
        "# 다양한 분류모델을 설정해서 비교할 수 있다.\n",
        "    classifier_name = trial.suggest_categorical('classifier', ['SVC', 'RandomForest'])\n",
        "    #분류 모델이 SVC일 때\n",
        "    if classifier_name == 'SVC':\n",
        "        svc_c = trial.suggest_loguniform('svc_c', 1e-10, 1e10)\n",
        "        classifier_obj = sklearn.svm.SVC(C=svc_c, gamma='auto')\n",
        "    \n",
        "    #분류모델이 랜덤포레스트일 때\n",
        "    else:\n",
        "        rf_max_depth = int(trial.suggest_loguniform('rf_max_depth', 2, 32))\n",
        "        classifier_obj = sklearn.ensemble.RandomForestClassifier(max_depth=rf_max_depth, n_estimators=10)\n",
        "    \n",
        "    accuracy = cross_val_score(classifier_obj, x, y, cv = 4).mean()\n",
        "    return accuracy\n",
        "\n",
        "# 3. study 오브젝트 생성하고 목적함수 최적화하는 단계\n",
        "# 여기서는 목적함수를 정확도로 설정했기 때문에 최대화를 목표로 하고 있지만, 손실함수의 경우 direction='minimize'로 설정\n",
        "study = optuna.create_study(direction='maximize')\n",
        "# 반복 시행 횟수(trial)는 200번으로\n",
        "study.optimize(objective, n_trials=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqgIzzzV8MyX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}