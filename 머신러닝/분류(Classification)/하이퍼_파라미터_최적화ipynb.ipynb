{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "하이퍼 파라미터 최적화ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLR3HxPj1fep"
      },
      "source": [
        "# iris_data 파라미터 최적화 방법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb3PfgOw7DUI"
      },
      "source": [
        "#import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "#Loading iris dataset from sklearn\n",
        "iris = load_iris()\n",
        "\n",
        "#independent feautres\n",
        "X = iris.data\n",
        "\n",
        "# target features\n",
        "y = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7SWSd_N7VJI"
      },
      "source": [
        "#import XGboost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#Defining XGB Classification model\n",
        "clf = XGBClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b4MwEbA7c6o"
      },
      "source": [
        "# 1.Grid SearchCV\n",
        "- 사용자가 하이퍼 파라미터마다 몇가지 값을 가진 리스트를 입력하면, 가능한 하이퍼 파라미터의 경우의 수마다 예측 성능을 측정하여 사용자가 일일이 하이퍼 파라미터를 설정하고, 예측 성능을 비교하여 최적의 파라미터를 찾는 수고를 줄이고 이 과정을 한꺼번에 진행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8j47oJZ7cYv"
      },
      "source": [
        "#Importing packages from sklearn\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "\n",
        "#defining a set of values as a dictionary for hyperparameters\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\":[100,200,300,400],\n",
        "    \"max_depth\":[1,3,5,7],\n",
        "    \"reg_lambda\":[.01,.1,.5]    \n",
        "}\n",
        "\n",
        "#declaring GridSearchCV model\n",
        "\n",
        "model = model_selection.GridSearchCV(\n",
        "    estimator = clf,\n",
        "    param_grid = param_grid,\n",
        "    scoring = 'accuracy',\n",
        "    verbose = 10,\n",
        "    n_jobs = 1,\n",
        "    cv = 5    \n",
        ")\n",
        "\n",
        "#fitting values to the gridsearchcv model\n",
        "\n",
        "model.fit(X,y)\n",
        "#printing the best possible values to enhance accuracy\n",
        "print(model.best_params_)\n",
        "print(model.best_estimator_)\n",
        "#printing the best score\n",
        "print(model.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utwaZq0g7o7o"
      },
      "source": [
        "# 2. RandomizedSearchCV\n",
        "\n",
        "-그리드 서치에서는 grid_param과 같이 매개변수마다 특정 값을 지정해주었습니다. 만약에 변수 범위가 너무 다양하다면 하나하나 작성해주는게 너무 힘들다.\n",
        "\n",
        "- 하이퍼 파라미터 검색 반영이 너무 클때 사용하는 방식이 Randomized Search입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y6qy6zT7nxh"
      },
      "source": [
        "#defining a set of values as a dictionary for hyperparameters\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\":[100,200,300,400],\n",
        "    \"max_depth\":[1,3,5,7],\n",
        "    \"reg_lambda\":[.01,.1,.5]    \n",
        "}\n",
        "\n",
        "#declaring RandomizedSearchCV model\n",
        "\n",
        "model = model_selection.RandomizedSearchCV(\n",
        "    estimator = clf,\n",
        "    param_distributions = param_grid,\n",
        "    scoring = 'accuracy',\n",
        "    verbose = 10,\n",
        "    n_jobs = 1,\n",
        "    cv = 5,\n",
        "    n_iter=10\n",
        ")\n",
        "\n",
        "#fitting values to the RandomizedSearchCV model\n",
        "\n",
        "model.fit(X,y)\n",
        "\n",
        "#printing the best possible values to enhance accuracy\n",
        "\n",
        "print(model.best_params_)\n",
        "print(model.best_estimator_)\n",
        "#printing the best score\n",
        "print(model.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6wKNMcY2t13"
      },
      "source": [
        "## 3. Bayesian optimization\n",
        "\n",
        "Bayesian Optimization 은 어느 입력값(x)를 받는 미지의 목적 함수(f(x))를 상정하여,\n",
        "\n",
        "해당 함숫값(f(x))을 최대로 만드는 최적해를 찾는 것을 목적으로 합니다.\n",
        "\n",
        "즉, 목적 함수(탐색대상함수)와 하이퍼파라미터 쌍(pair)을 대상으로 Surrogate Model(대체 모델) 을 만들고,\n",
        "순차적으로 하이퍼 파라미터를 업데이트해 가면서 평가를 통해 최적의 하이퍼파라미터 조합을 탐색합니다.\n",
        "이 때의 목점 함수를 black-box function 이라고 합니다.\n",
        "Bayesian Optimization 에는 두 가지 필수 요소가 존재합니다.\n",
        "\n",
        "먼저 Surrogate Model 은, 현재까지 조사된 입력값-함숫결과값 점들 (x1, f(x1)),...,(xt, f(xt)) 을 바탕으로, 미지의 목적 함수의 형태에 대한 확률적인 추정을 수행하는 모델을 지칭합니다. 그리고 Acquisition Function 은, 목적 함수에 대한 현재까지의 확률적 추정 결과를 바탕으로, ‘최적 입력값을 찾는 데 있어 가장 유용할 만한’ 다음 입력값 후보를 추천해 주는 함수를 지칭합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19z0i4Yh22N2"
      },
      "source": [
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb33tsP%2FbtraMpvxJG0%2FSn7uQK7k910IQ7cP3ZM9vk%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwaG9j8J24MH"
      },
      "source": [
        "- 대략적인 수행 과정\n",
        "\n",
        "  - 위의 파란색 선은, 우리가 찾으려고 하는 목적함수 f(x) 를 나타내고,\n",
        "검정색 점선은, 지금까지 관측한 데이터를 바탕으로 우리가 예측한 estimated function 을 의미합니다.\n",
        "\n",
        "  - 검정색 점선 주변에 있는 파란 영역은, 목적함수 f(x) 가 존재할만한 confidence bound(function의 variance) 를 의미합니다.\n",
        "\n",
        "  - 밑에 있는 EI(x) 는, Acquisition function 을 의미하며, 다음 입력값 후보 추천 시 사용됩니다.\n",
        "\n",
        "  - Acquisition function 값이 컸던 지점을 확인하고, 해당 지점의 hyperparameter 를 다음 입력 값으로 사용합니다.\n",
        "  - hyperparamter 에 따라 estimated function 을 계속 update 하면, estimation function 과 목적 함수 f(x) 가 흡사해집니다. 관측한 지점 중 best point 을 argmax f(x) 로 선택합니다.\n",
        "\n",
        "- 자세한 수행 과정\n",
        "\n",
        " 1. 입력값, 목적 함수 및 그 외 설정값들을 정의합니다.\n",
        "\n",
        "  1. 입력값 x : 여러가지 hyperparameter\n",
        "\n",
        "  2. 목적 함수 f(x) : 설정한 입력값을 적용해 학습한, 딥러닝 모델의 성능 결과 수치(e.g. 정확도)\n",
        "  3. 입력값 x 의 탐색 대상 구간 : (a,b)\n",
        "  \n",
        "  4. 입력값-함숫결과값 점들의 갯수 : n\n",
        "\n",
        "  5. 조사할 입력값-함숫결과값 점들의 갯수 : N\n",
        "\n",
        "- 설정한 탐색 대상 구간 (a,b) 내에서 처음 n 개의 입력값들을 랜덤하게 샘플링하여 선택합니다.\n",
        "\n",
        "- 선택한 n 개의 입력값 x1, x2, ..., xn 을 각각 모델의 hyperparameter 로 설정하여 딥러닝 모델을 학습한 뒤, 학습이 완료된 모델의 성능 결과 수치를 계산합니다.\n",
        "\n",
        "  - 이들을 각각 함숫결과값 f(x1), f(x2), ..., f(xn) 으로 간주합니다.\n",
        "입력값-함숫결과값 점들의 모음 (x1, f(x1)), (x2, f(x2)), ..., (xn, f(xn)) 에 대하여 Surrogate Model 로 확률적 추정을 수행합니다.\n",
        "\n",
        "- 조사된 입력값-함숫결과값 점들이 총 N 개에 도달할 때까지, 아래의 과정을 반복적으로 수행합니다.\n",
        "\n",
        "  - 기존 입력값-함숫결과값 점들의 모음 (x1, f(x1)),(x2, f(x2)), ..., (xt, f(xt)) 에 대한 Surrogate Model 의 확률적 추정 결과를 바탕으로, 입력값 구간 (a,b) 내에서의 EI 의 값을 계산하고, 그 값이 가장 큰 점을 다음 입력값 후보 x1 로 선정합니다.\n",
        "\n",
        "  - 다음 입력값 후보 x1 를 hyperparameter 로 설정하여 딥러닝 모델을 학습한 뒤, 학습이 완료된 모델의 성능 결과 수치를 계산하고, 이를 f(x1) 값으로 간주합니다.\n",
        "\n",
        "  - 새로운 점 (x2, f(x2)) 을 기존 입력값-함숫결과값 점들의 모음에 추가하고, 갱신된 점들의 모음에 대하여 Surrogate Model 로 확률적 추정을 다시 수행합니다.\n",
        "\n",
        "\n",
        "- 총 N 개의 입력값-함숫결과값 점들에 대하여 확률적으로 추정된 목적 함수 결과물을 바탕으로, 평균 함수 μ(x) 을 최대로 만드는 최적해를 최종 선택합니다. 추후 해당값을 hyperparameter 로 사용하여 딥러닝 모델을 학습하면, 일반화 성능이 극대화된 모델을 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxVJ6Jqo6gIr"
      },
      "source": [
        "def black_box_function(x, y):\n",
        "    \"\"\"Function with unknown internals we wish to maximize.\n",
        "\n",
        "    This is just serving as an example, for all intents and\n",
        "    purposes think of the internals of this function, i.e.: the process\n",
        "    which generates its output values, as unknown.\n",
        "    \"\"\"\n",
        "    return -x ** 2 - (y - 1) ** 2 + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RXmgYFFaUiq"
      },
      "source": [
        "!pip install bayesian-optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FtyWCLI2bBy"
      },
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'x': (2, 4), 'y': (-3, 3)}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=black_box_function,\n",
        "    pbounds=pbounds,\n",
        "    random_state=1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WjShm_m6jBS"
      },
      "source": [
        "- n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
        "\n",
        "- init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekgECzuR6hcb"
      },
      "source": [
        "optimizer.maximize(\n",
        "    init_points=2,\n",
        "    n_iter=3,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xIv60X06hfR"
      },
      "source": [
        "print(optimizer.max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmCE8Ja12bEZ"
      },
      "source": [
        "for i, res in enumerate(optimizer.res):\n",
        "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ3Fl4lq6sNA"
      },
      "source": [
        "optimizer.set_bounds(new_bounds={\"x\": (-2, 3)})\n",
        "\n",
        "optimizer.maximize(\n",
        "    init_points=0,\n",
        "    n_iter=5,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJICPdtp8E2Q"
      },
      "source": [
        "# 4.Hyperopt\n",
        "\n",
        "- HyperOpt 는 하이퍼 파라미터 최적화를 위해 검색 공간 , 손실 함수 , 최적화 알고리즘 및 기록 ( 점수, 구성 ) 을 저장하기 위한 데이터베이스의 4 가지 필수 구성 요소가 필요합니다 . 검색 공간은 연속하고, 볼록 함수에 의해 결정될 것이다.\n",
        "\n",
        "참고 : https://ichi.pro/ko/hyperopt-beijian-choejeoghwaleul-giban-eulo-han-haipeo-palamiteo-tyuning-140338828128041"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBKs-S6ibIAW"
      },
      "source": [
        "!pip install scikit-optimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHCb9jCY7xZg"
      },
      "source": [
        "#importing packages \n",
        "\n",
        "from hyperopt import hp,fmin, tpe, Trials\n",
        "\n",
        "from hyperopt.pyll.base import scope\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "from skopt import space\n",
        "\n",
        "from skopt import gp_minimize\n",
        "\n",
        "#defining a method that will perfrom a 5 split cross validation over\n",
        "\n",
        "#dataset and and will produce the optimum value of the accuracy\n",
        "\n",
        "def optimize(params, x,y):\n",
        "\n",
        "    clf = XGBClassifier(**params)\n",
        "\n",
        "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    for idx in kf.split(X=x,y=y):\n",
        "\n",
        "        train_idx,test_idx = idx[0],idx[1]\n",
        "\n",
        "        xtrain = x[train_idx]\n",
        "\n",
        "        ytrain = y[train_idx]\n",
        "\n",
        "        xtest = x[test_idx]\n",
        "\n",
        "        ytest = y[test_idx]\n",
        "\n",
        "        clf.fit(xtrain,ytrain)\n",
        "\n",
        "        preds =  clf.predict(xtest)\n",
        "\n",
        "        fold_acc = metrics.accuracy_score(ytest,preds)\n",
        "\n",
        "        accuracies.append(fold_acc)\n",
        "\n",
        "    return -1.0 * np.mean(accuracies)\n",
        "\n",
        "#defining a set of values as hp for hyperparameters\n",
        "\n",
        "param_space = {\n",
        "\n",
        "    \"max_depth\" : scope.int(hp.quniform(\"max_depth\",3,20, 1)) ,\n",
        "\n",
        "    \"min_child_weight\" : scope.int(hp.quniform(\"min_child_weight\",1,8, 1)),\n",
        "\n",
        "    \"n_estimators\": scope.int(hp.quniform(\"n_estimators\",100,1500,1)),\n",
        "\n",
        "    'learning_rate': hp.uniform(\"learning_rate\",0.01,1),\n",
        "\n",
        "    'reg_lambda': hp.uniform(\"reg_lambda\",0.01,1),\n",
        "\n",
        "    'gamma': hp.uniform(\"gamma\",0.01,1),\n",
        "\n",
        "    'subsample': hp.uniform(\"subsample\",0.01,1)\n",
        "\n",
        "    }\n",
        "\n",
        "#defiing optimization_fuction as partial and calling optimize within it\n",
        "\n",
        "optimization_fuction = partial(optimize,x = X, y = y) \n",
        "\n",
        "trials = Trials()\n",
        "\n",
        "#Getting the optimum values for hyperparameters\n",
        "\n",
        "result = fmin(\n",
        "\n",
        "    fn = optimization_fuction,\n",
        "\n",
        "    space = param_space,\n",
        "\n",
        "    algo = tpe.suggest,\n",
        "\n",
        "    max_evals = 15,\n",
        "\n",
        "    trials = trials\n",
        "\n",
        ")\n",
        "\n",
        "#Printing the best hyperparemeter set\n",
        "\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQWHSPbq8KHB"
      },
      "source": [
        "# 5. Optuna\n",
        "\n",
        "참고 : https://rosypark.tistory.com/153"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rrKe5An7ITr"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0vcDnBi8JD3"
      },
      "source": [
        "#importing packages\n",
        "\n",
        "import optuna\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "#defining a method that will perfrom a 5 split cross validation over\n",
        "\n",
        "#dataset and and will produce the optimum value of the accuracy\n",
        "\n",
        "def optimize(trial, x,y):\n",
        "\n",
        "    #parameter set is declare within function\n",
        "\n",
        "    reg_lambda = trial.suggest_uniform('reg_lambda',0.01,1)\n",
        "\n",
        "    n_estimators = trial.suggest_int('n_estimators',100,1500)\n",
        "\n",
        "    max_depth = trial.suggest_int('max_depth',3,15)\n",
        "\n",
        "    max_features = trial.suggest_uniform('max_features',0.01,1)\n",
        "\n",
        "    clf = XGBClassifier(\n",
        "\n",
        "    n_estimators= n_estimators,\n",
        "\n",
        "    reg_lambda=reg_lambda,\n",
        "\n",
        "    max_depth=max_depth,\n",
        "\n",
        "    max_features= max_features)\n",
        "\n",
        "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    for idx in kf.split(X=x,y=y):\n",
        "\n",
        "        train_idx,test_idx = idx[0],idx[1]\n",
        "\n",
        "        xtrain = x[train_idx]\n",
        "\n",
        "        ytrain = y[train_idx]\n",
        "\n",
        "        xtest = x[test_idx]\n",
        "\n",
        "        ytest = y[test_idx]\n",
        "\n",
        "        clf.fit(xtrain,ytrain)\n",
        "\n",
        "        preds =  clf.predict(xtest)\n",
        "\n",
        "        fold_acc = metrics.accuracy_score(ytest,preds)\n",
        "\n",
        "        accuracies.append(fold_acc)\n",
        "\n",
        "    return -1.0 * np.mean(accuracies)\n",
        "\n",
        "#defiing optimization_fuction as partial and calling optimize within it\n",
        "\n",
        "optimization_fuction = partial(optimize,x = X, y = y) \n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "\n",
        "#Printing the best hyperparemeter set\n",
        "\n",
        "study.optimize(optimization_fuction, n_trials=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYUaw8YfbuUL"
      },
      "source": [
        "# 시각화\n",
        "optuna.visualization.plot_optimization_history(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MUidiidbypj"
      },
      "source": [
        "# 파라미터들관의 관계\n",
        "optuna.visualization.plot_parallel_coordinate(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDU1iODVb42t"
      },
      "source": [
        "# 각 파라미터들의 상관관계\n",
        "optuna.visualization.plot_contour(\n",
        "    study\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rmsRlgXcQPJ"
      },
      "source": [
        "# 하이퍼파라미터 중요도\n",
        "optuna.visualization.plot_param_importances(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcXuNtGJ8XVh"
      },
      "source": [
        "# 6.Pycaret"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqlGmSqx8aeh"
      },
      "source": [
        "!pip install pycaret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G4-MhRH8W3R"
      },
      "source": [
        "import optuna\n",
        "import sklearn\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# 1. 최소화/최대화할 목적함수 정의\n",
        "def objective(trial):\n",
        "    iris = sklearn.datasets.load_iris()\n",
        "    x, y = iris.data, iris.target\n",
        "\n",
        "# 2. trial object로 하이퍼파라미터 값 추천\n",
        "# 다양한 분류모델을 설정해서 비교할 수 있다.\n",
        "    classifier_name = trial.suggest_categorical('classifier', ['SVC', 'RandomForest'])\n",
        "    #분류 모델이 SVC일 때\n",
        "    if classifier_name == 'SVC':\n",
        "        svc_c = trial.suggest_loguniform('svc_c', 1e-10, 1e10)\n",
        "        classifier_obj = sklearn.svm.SVC(C=svc_c, gamma='auto')\n",
        "    \n",
        "    #분류모델이 랜덤포레스트일 때\n",
        "    else:\n",
        "        rf_max_depth = int(trial.suggest_loguniform('rf_max_depth', 2, 32))\n",
        "        classifier_obj = sklearn.ensemble.RandomForestClassifier(max_depth=rf_max_depth, n_estimators=10)\n",
        "    \n",
        "    accuracy = cross_val_score(classifier_obj, x, y, cv = 4).mean()\n",
        "    return accuracy\n",
        "\n",
        "# 3. study 오브젝트 생성하고 목적함수 최적화하는 단계\n",
        "# 여기서는 목적함수를 정확도로 설정했기 때문에 최대화를 목표로 하고 있지만, 손실함수의 경우 direction='minimize'로 설정\n",
        "study = optuna.create_study(direction='maximize')\n",
        "# 반복 시행 횟수(trial)는 200번으로\n",
        "study.optimize(objective, n_trials=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5_5kiF_-F8i"
      },
      "source": [
        "Ray tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqgIzzzV8MyX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}